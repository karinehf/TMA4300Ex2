---
title: "TMA4300Ex2"
author: "Karine H. Foss & August S. Mathisen"
date: "18 2 2019"
output: pdf_document
---
Comments: 

*  Need to explain better what $q(\eta)$ is. This is proposal distribution?
*  Which elements in the canonical form of mvnorm are the precision matrix etc?
*  ex3: says u and v in text; should be eta?
*  ex3: should we remove burn-in period before performing test? (and acf?)
*  ex5: says exp(u), but should be eta??


```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)

```

```{r, eval=FALSE, include=FALSE}
#Installing packages
#install.packages(c("fields","spam"))
#install.packages("INLA", repos = c(getOption("repos"),
#                 INLA="https://inla.r-inla-download.org/R/stable"), #dep=TRUE)
```

```{r, child = 'text/introduction.Rmd'}
```


```{r}
#Loading libraries
library(ggplot2)
library(gridExtra)
library(spam) # load the data
str(Oral) # see structure of data
#???data.frame???: 544 obs. of 3 variables:
# $ Y : int 18 62 44 12 18 27 20 29 39 21 . . .
# $ E : num 16.4 45.9 44.7 16.3 26.9 . . .
# $ SMR: num 1.101 1.351 0.985 0.735 0.668 . . .
attach(Oral) # allow direct referencing to Y and E
# load some libraries to generate nice map plots
library(fields, warn.conflict=FALSE)
library(colorspace)
source('additionalFiles/dmvnorm.R')


col <- diverge_hcl(8) # blue - red
# use a function provided by spam to plot the map together with the mortality rates
germany.plot(Oral$Y/Oral$E, col=col, legend=TRUE)
load('additionalFiles/tma4300_ex2_Rmatrix.Rdata')

#Set seed so that the task can be reproduced
set.seed(42)
```

#Exercise 1: Derivations
```{r, child = 'text/task1.Rmd'}
```

#Exercise 2: Implementation of the MCMC sampler
```{r, child="text/task2theory.Rmd"}
```

```{r}
#Define functions to be used in the MCMC

#Changed from z to exp(z) here
c_func = function(z, E){
  exp(z)*E
}

b_func = function(z, y, E){
  y + c_func(z,E)*(z-1)
}

drawKappaU = function(n, alpha_u, beta_u, u, R){
  rgamma(1, shape = (n-1)/2+alpha_u, rate = beta_u + 0.5*t(u)%*%R%*%u)
}

drawKappaV = function(n, alpha_v, beta_v, eta, u){
  rgamma(1, shape = n/2+alpha_v, rate = beta_v+0.5*sum((eta-u)^2))
}

drawU = function(n, kappa_u, kappa_v, eta, R){
  t(rmvnorm.canonical(1, kappa_v*eta, kappa_u*R+diag.spam(kappa_v, n, n)))
}

drawEta = function(n, z, y, kappa_v, kappa_u, u, E){
  t(rmvnorm.canonical(1, kappa_v*u + b_func(z,y,E), diag.spam(as.vector(c_func(z,E)))+ diag.spam(kappa_v, n, n)))
}

logFullCondEta = function(eta, kappa_v, u, y, E){
  -kappa_v*0.5*t(eta)%*%eta+t(eta)%*%u*kappa_v + t(eta)%*%y-t(exp(eta))%*%E
}

#Not in use anymore:
calculateLogAcceptanceProb = function(etaProp, etaPrev, kappa_v, E){
  c_prop = c_func(etaProp, E)
  c_prev = c_func(etaPrev, E)
  logProb = (sum(log(kappa_v+c_prop))-sum(log(kappa_v+c_prev)))/2+t(c_prev*etaProp)%*%(0.5*etaProp-etaPrev)-t(c_prop*etaPrev)%*%(0.5*etaPrev-etaProp)+sum(c_prev)-sum(c_prop)+t(etaProp)%*%c_prev-t(etaPrev)%*%c_prop
}

```

Now we implement the MCMC algorithm. It updates each of the components at a time. That is, it follows the Gibbs sampler. For $\boldsymbol\eta$, a metropolis-hastings step is needed, as the full conditional is not on a form that can easily be sampled from in itself. The proposal density is a gaussian density found from Taylor expansion around the value in the previous step.
```{r}
#MCMC function
MCMCOral = function(M, y, starting_values, alpha_u, beta_u, alpha_v, beta_v, E, R){
  #Get computing time
  time_start = Sys.time()
  
  #Fetching starting values
  eta_prev = starting_values[['eta']]
  u = starting_values[['u']]
  kappa_u = starting_values[['kappa_u']]
  kappa_v = starting_values[['kappa_v']]
  
  n = length(y)
  
  #Create matrices to contain all steps
  eta_values = matrix(nrow = n, ncol = M)
  u_values = matrix(nrow = n, ncol = M)
  kappa_u_vals = vector(length = M)
  kappa_v_vals = vector(length = M)
  
  eta_values[,1] = eta_prev
  u_values[,1] = u
  kappa_v_vals[1] = kappa_v
  kappa_u_vals[1] = kappa_u
  
  acceptance_rates = vector(length = M-1)
  num_accepted = 0
  
  for(i in 2:M){
    #Generate next step for u and kappas
    kappa_u = drawKappaU(n, alpha_u, beta_u, u, R)
    kappa_v = drawKappaV(n, alpha_v, beta_v, eta_prev, u)
    u = drawU(n, kappa_u, kappa_v, eta_prev, R)
    
    #prpose eta
    eta = drawEta(n, eta_prev, y, kappa_v, kappa_u, u, E)
    
    #generate acceptance variable
    decision_var = runif(1)
    
    #check for acceptance
    #log_acceptance = calculateLogAcceptanceProb(eta, eta_prev, kappa_v, E)
    p_eta = logFullCondEta(eta, kappa_v, u, y, E)
    p_eta_prev = logFullCondEta(eta_prev, kappa_v, u, y, E)
    q_eta = dmvnorm.canonical(eta, kappa_v*u + b_func(eta_prev,y,E), diag.spam(as.vector(c_func(eta_prev,E)+kappa_v)), log=TRUE)
    q_eta_prev = dmvnorm.canonical(eta_prev, kappa_v*u + b_func(eta,y,E), diag.spam(as.vector(c_func(eta,E))+kappa_v), log=TRUE)
    log_acceptance = p_eta-p_eta_prev+q_eta_prev-q_eta
    
    
    #Using log_acceptance further
    acceptance_rates[i] = min(1, exp(log_acceptance))
    
    #Changed from log_acceptance to acceptance_rates[i]:
    if(decision_var<acceptance_rates[i]){
      eta_prev = eta
      num_accepted = num_accepted +1
    }
    
    #Update value matrix
    eta_values[,i] = eta_prev
    u_values[,i] = u
    kappa_v_vals[i] = kappa_v
    kappa_u_vals[i] = kappa_u
  }
  time_end = Sys.time()
  #Return all values in a list
  return(list(eta = eta_values, u = u_values, kappa_u =kappa_u_vals, kappa_v=kappa_v_vals, acceptance_rates =  acceptance_rates, num_accepted=num_accepted, time = difftime(time_end, time_start,units = 'secs')))
}
```

Known values, the number of samples and initial values are set before we run the sampler.
```{r}
#Set parameters
M = 50000
y = Oral$Y
E = Oral$E
alpha_u = 1
alpha_v = 1
beta_u = 0.01
beta_v = 0.01

#Set initial values
n = length(y)
u_start = matrix(0,ncol = 1, nrow = n)
eta_start = matrix(0,nrow = n, ncol=1)
kappa_u_start = 100
kappa_v_start = 100
starting_values = list(eta = eta_start, u = u_start, kappa_u = kappa_u_start, kappa_v = kappa_v_start)
```

```{r, eval = TRUE}
#Run function.
set.seed(4300)
#samples = MCMCOral(M,y,starting_values,alpha_u,beta_u,alpha_v,beta_v,E,R)

#Save and load, so we don't need to do it over and over again
#save(samples, file = 'MCMCsamples.RData', ascii=TRUE)
load('MCMCsamples.RData')
#M = length(samples$kappa_u)
```


#Exercise 3: Convergence diagnostics
We will now do some analysis of the result of the MCMC. First, we will obtain some diagnostic summaries for a set of the parameters. We will try to conclude whether the chain has converged to the target/stationary distribuition.

##a) Trace plots
First, we look at the trace plots for $\kappa_u$ and $\kappa_v$. 
Figure **x** shows all samples, including the burn-in period in the beginning. In figure **y** the burn-in period is disregarded, to show the variability in the parameters, and figure **z** shows just a small part of the samples, and the individual steps in the algorithm is more visible here.
```{r}
#x
#Trace plots Kappa
start = 1
trace.kappa_u = 
  ggplot(data.frame(kappa = samples$kappa_u[start:M]), aes(x=start:M, y = kappa)) + geom_line()+ ggtitle("MCMC trace plots: Kappa") + theme(plot.title = element_text(hjust = 0.5)) + xlab('Iterations') + ylab('kappa_u')

trace.kappa_v = 
  ggplot(data.frame(kappa = samples$kappa_v[start:M]), aes(x=start:M, y = kappa)) + geom_line() + xlab('Iterations') + ylab('kappa_v')

trace.kappa_plot = grid.arrange(grobs = list(trace.kappa_u, trace.kappa_v), ncol = 1)
```

```{r}
#y
#Trace plots Kappa
start = 1000
trace.kappa_u = 
  ggplot(data.frame(kappa = samples$kappa_u[start:M]), aes(x=start:M, y = kappa)) + geom_line()+ ggtitle("MCMC trace plots: Kappa") + theme(plot.title = element_text(hjust = 0.5)) + xlab('Iterations') + ylab('kappa_u')

trace.kappa_v = 
  ggplot(data.frame(kappa = samples$kappa_v[start:M]), aes(x=start:M, y = kappa)) + geom_line() + xlab('Iterations') + ylab('kappa_v')

trace.kappa_plot = grid.arrange(grobs = list(trace.kappa_u, trace.kappa_v), ncol = 1)
```

```{r}
#z
#Trace plots Kappa
start = 1000
stop = 1500
trace.kappa_u = 
  ggplot(data.frame(kappa = samples$kappa_u[start:stop]), aes(x=start:stop, y = kappa)) + geom_line()+ ggtitle("MCMC trace plots: Kappa") + theme(plot.title = element_text(hjust = 0.5)) + xlab('Iterations') + ylab('kappa_u')

trace.kappa_v = 
  ggplot(data.frame(kappa = samples$kappa_v[start:stop]), aes(x=start:stop, y = kappa)) + geom_line() + xlab('Iterations') + ylab('kappa_v')

trace.kappa_plot = grid.arrange(grobs = list(trace.kappa_u, trace.kappa_v), ncol = 1)
```

**u and eta or u and v??**
Since the vectors of $\boldsymbol\eta$ and $\boldsymbol\u$ are of high dimensions, trace plots are only made for a few, randomly chosen components.

```{r}
#Trace plots somecomponents of u and eta
#Choose components: 
set.seed(4301)
comp = ceiling(n*runif(3))
trace.u = list()
trace.eta = list()

#labels
lab = paste("comp", comp)

#Make first plots with title
  trace.u[[1]] = 
  ggplot(data.frame(u = samples$u[comp[1],]), aes(x=1:M, y = u)) + geom_line()+ ggtitle("MCMC trace plots: u components") + theme(plot.title = element_text(hjust = 0.5)) + xlab('Iterations') + ylab(lab[1])

trace.eta[[1]] = 
  ggplot(data.frame(eta = samples$eta[comp[1],]), aes(x=1:M, y = eta)) + geom_line()+ ggtitle("MCMC trace plots: eta components") + theme(plot.title = element_text(hjust = 0.5)) + xlab('Iterations')+ ylab(lab[1])

#Next plots do not need title
for (i in 2:length(comp)){
  trace.u[[i]] = 
  ggplot(data.frame(u = samples$u[comp[i],]), aes(x=1:M, y = u)) + geom_line() + xlab('Iterations')+ ylab(lab[i])

trace.eta[[i]] = 
  ggplot(data.frame(eta = samples$eta[comp[i],]), aes(x=1:M, y = eta)) + geom_line() + xlab('Iterations')+ ylab(lab[i])
}

trace.u_plot = grid.arrange(grobs = trace.u, ncol = 1)
trace.eta_plot = grid.arrange(grobs = trace.eta, ncol = 1)
```

$\boldsymbol\eta$ and $\boldsymbol\u$ seem to have very small burn-in period.
To be on the safe side, in the further analysis, the first $1000$ samples are disregarded.
```{r}
#Define burn-in area
burnins = 1:1000
```

##b) Autocorrelation plots

Autocorrelation plots are made for the same components as the trace plots.
```{r}
acf.kappa_u = acf(samples$kappa_u[-burnins], plot = FALSE)
acf.kappa_v = acf(samples$kappa_v[-burnins], plot = FALSE)

plot1 = ggplot(data.frame(acf = acf.kappa_u$acf, lag = acf.kappa_u$lag), aes(x = lag, y = acf)) + geom_col(width = 0.2) + ggtitle("ACF plots: Kappa") + theme(plot.title = element_text(hjust = 0.5)) + ylab('acf kappa_u')

plot2 = ggplot(data.frame(acf = acf.kappa_v$acf, lag = acf.kappa_v$lag), aes(x = lag, y = acf)) + geom_col(width = 0.2) + ylab('acf kappa_v')

acf.kappa_plot = grid.arrange(grobs = list(plot1, plot2), ncol = 1)
```

```{r}
#acf for u and eta
plot_u = list()
plot_eta = list()

#Make first plots with title
acf.u = acf(samples$u[comp[1],-burnins], plot = FALSE)
plot_u[[1]] = ggplot(data.frame(acf = acf.u$acf, lag = acf.u$lag), aes(x = lag, y = acf)) + geom_col(width = 0.2) + ggtitle("ACF plots: u components") + theme(plot.title = element_text(hjust = 0.5)) + ylab(lab[1])

acf.eta = acf(samples$eta[comp[1],-burnins], plot = FALSE)
plot_eta[[1]] = ggplot(data.frame(acf = acf.eta$acf, lag = acf.eta$lag), aes(x = lag, y = acf)) + geom_col(width = 0.2) + ggtitle("ACF plots: eta components") + theme(plot.title = element_text(hjust = 0.5)) + ylab(lab[1])

#Next plots do not need title
for (i in 2:length(comp)){
  acf.u = acf(samples$u[comp[i],-burnins], plot = FALSE)
  plot_u[[i]] = ggplot(data.frame(acf = acf.u$acf, lag = acf.u$lag), aes(x = lag, y = acf)) + geom_col(width = 0.2) + ylab(lab[i])

  acf.eta = acf(samples$eta[comp[i],-burnins], plot = FALSE)
  plot_eta[[i]] = ggplot(data.frame(acf = acf.eta$acf, lag = acf.eta$lag), aes(x = lag, y = acf)) + geom_col(width = 0.2) + ylab(lab[i])
}

acf.u_plot = grid.arrange(grobs = plot_u, ncol = 1)
acf.eta_plot = grid.arrange(grobs = plot_eta, ncol = 1)

```
We see that the steps are quite correlated for the $\kappa$s. From the trace plot in figure **z**, this tendency can be spotted. The mixing does not seem to be very good; the steps are often quite small, so that it takes quite a lot of steps to explore the whole areas.

##Convergence test
geweke.diag() performs a test for convergence of the markov chain.
It takes one fraction of the beginning of the chain and one fraction of the end of the MCMC. The burn-in period is disregarded. If the chain has converged to the stationary distribution, the two means should be equal. The geweke test statistic is asymptotically standard normal distributed.
The null hypothesis is that the chain has converged.

```{r}
library(coda)

#Make data frame of samples
u_df = as.data.frame(t(samples$u))
eta_df = as.data.frame(t(samples$eta))

#Apply geweke test: 
gew.kappa_u = geweke.diag(samples$kappa_u[-burnins])
gew.kappa_v = geweke.diag(samples$kappa_v[-burnins])

gew.u = geweke.diag(u_df[-burnins,])
gew.eta = geweke.diag(eta_df[-burnins,])


p_u = 2*pnorm(abs(gew.u$z), lower.tail = FALSE)
p_eta = 2*pnorm(abs(gew.eta$z), lower.tail = FALSE)
p_kappa = c(2*pnorm(abs(gew.kappa_u$z), lower.tail = FALSE),
          2*pnorm(abs(gew.kappa_v$z), lower.tail = FALSE))

```


```{r, echo = FALSE}
cat('kappa_u:')
print(gew.kappa_u)
cat( '  kappa_u:     test stat',gew.kappa_u$z, ' p-value', p_kappa[1], '\n')
cat( '  kappa_v:     test stat',gew.kappa_v$z, ' p-value',p_kappa[2], '\n')

cat('  u comp', comp[1], ': test stat', gew.u$z[comp[1]], ' p-value', p_u[comp[1]], '\n')
cat('  u comp', comp[2], ': test stat', gew.u$z[comp[2]],' p-value', p_u[comp[2]], '\n')
cat('  u comp', comp[3], ': test stat', gew.u$z[comp[3]],' p-value', p_u[comp[3]], '\n')

cat('eta comp', comp[1], ': test stat', gew.eta$z[comp[1]],' p-value', p_eta[comp[1]],'\n')
cat('eta comp', comp[2], ': test stat', gew.eta$z[comp[2]],' p-value', p_eta[comp[2]],'\n')
cat('eta comp', comp[3], ': test stat', gew.eta$z[comp[3]],' p-value', p_eta[comp[3]],'\n')

```

We make a histogram of all p-values to see whether they seem to be uniformly distributed, as they would be if the null hypothesis is true and the chain has converged.
```{r}
ggplot(data = data.frame(p_vals = c(p_u,p_eta, p_kappa), var = c(rep(1, length = length(p_u)), rep(2,length = length(p_eta)), rep(3, length = length(p_kappa))))) + geom_histogram(aes(x = p_vals, y = ..density..))

```

**Conclusion?**
For some components, isolated, we will conclude from the geweke test that the samples have not converged to the target distribution. However, overall, the p-values seem at least not far from uniformly distributed. From the trace plots, it looks like the chain has converged. The autocorrelation plots show quite high correlation for some components, but some correlation cannot be avoided, as the proposed steps depend on the previous steps.
Al in all, it seems the MCMC has converged, but of course, we cannot be completely sure.

#Exercise 4: Effective sample set

```{r}
effSize.kappa_u = effectiveSize(samples$kappa_u[-burnins])
effSize.kappa_v = effectiveSize(samples$kappa_v[-burnins])
```

**interpret the values**
These values mean that out of the $M$ samples of the MCMC, we "effectively" have $939$ and $348$ samples from the posterior distributions of $\kappa_u$ and $\kappa_v$, respectively. These are **very low!** values.**??**

**How could the MCMC be changed to increas the effective sample size?**

```{r}
comp_time = as.numeric(samples$time)
#comp_time = 100 #For testing when the time was not computed
relEffSize.kappa_u = effSize.kappa_u/comp_time
relEffSize.kappa_v= effSize.kappa_v/comp_time
```

```{r, echo = FALSE}
cat('Relative effective sample size for kappa_u is ', relEffSize.kappa_u, '\n')
cat('Relative effective sample size for kappa_v is ', relEffSize.kappa_v, '\n')
```
The relaive effective sample size is the effective sample size obtained per second of running the MCMC. This could be interesting, as it gives us some meaure of how efficient the MCMC algorithm is. Low computer time is not good in itself if the algorithm does not draw enough samples from the target distribution. Likewise, large effective sample size is of course desireble, but if the algorithm has to run for an unreasonable long time, some improvement of the algorithm or sampling approach should maybe be considered. 

These numbers in themselves might not be too informative, but in relation to other approaches, they will give some measure of which to use.

#Exercise 5: Interpretation of result
**u or eta?**

```{r}
post_median = exp(apply(samples$u, 1, median))

germany.plot(post_median, col=col, legend=TRUE)
```

We see that the general tendencies from the data ($y_i/E_i$) in the beginning of the report are still present in this posterior distribution, but that the posterior distribution of the expected number of cancers per region is smoother across region. **meeeh**


#Exercise 6: INLA

##a)
R-INLA can also be used to approximate oral cancer in Germany.
```{r}
#Build the same model using INLA
library(INLA)
g <- system.file("demodata/germany.graph", package="INLA")

#Known values and parameters (copied from task2)
y = Oral$Y
E = Oral$E
alpha_u = 1
alpha_v = 1
beta_u = 0.01
beta_v = 0.01
n = length(y)
OralData = cbind(Oral, Region = Germany$region, Region.struct = Germany$region)
```

```{r}
INLAOral = inla(
  formula = Y ~ f(Region.struct, model = "besag", graph = g, hyper = list(prec=list(param=c(alpha_u,beta_u))), constr = FALSE) + 
    f(Region, model = "iid", hyper = list(prec=list(param=c(alpha_v,beta_v))))-1, 
 family = "poisson",
 E=E,
 data = OralData,
 control.compute = list(dic=TRUE)
)

```

```{r}
#Marginal posteriors and histograms
kappa_u_marginal = INLAOral$marginals.hyperpar$`Precision for Region.struct`
kappa_v_marginal = INLAOral$marginals.hyperpar$`Precision for Region`

kappa_u_df = as.data.frame(kappa_u_marginal[-(74:75),])
kappa_v_df = as.data.frame(kappa_v_marginal[-(70:75),])

ggplot(data.frame(kappa = samples$kappa_u[-burnins]), aes(kappa)) + 
  geom_histogram(aes(y=..density..), binwidth = 0.5,boundary = 0) + 
  geom_path(aes.inherit = FALSE, data = kappa_u_df, aes(x=x, y=y), col ='red') + 
  ggtitle("") +
 theme(plot.title = element_text(hjust = 0.5)) + xlab('x')


ggplot(data.frame(kappa = samples$kappa_v[-burnins]), aes(kappa)) + 
  geom_histogram(aes(y=..density..), binwidth = 10,boundary = 0) + 
  geom_path(aes.inherit = FALSE, data = kappa_v_df, aes(x=x, y=y), col ='red') + 
  ggtitle("") +
 theme(plot.title = element_text(hjust = 0.5)) + xlab('x')
#samma_det = inla.hyperpar(INLAOral)
```
```{r}
u_inla = INLAOral$marginals.random$Region.struct[comp]
v_inla = INLAOral$marginals.random$Region[comp]
v_MCMC = samples$eta-samples$u

u_inla_df = as.data.frame(u_inla[[1]])
v_inla_df = as.data.frame(v_inla[[1]])

u_plots = list()
v_plots = list()
binw = 0.02

u_plots[[1]] = ggplot(data.frame(samp = samples$u[comp[1],-burnins]), aes(samp)) + 
  geom_histogram(aes(y=..density..), binwidth = binw) + 
  geom_path(aes.inherit = FALSE, data = u_inla_df, aes(x=x, y=y), col ='red') + 
  ggtitle("") +
 theme(plot.title = element_text(hjust = 0.5)) + xlab('x')

v_plots[[1]] = ggplot(data.frame(samp = v_MCMC[comp[1],-burnins]), aes(samp)) + 
  geom_histogram(aes(y=..density..), binwidth = binw) + 
  geom_path(aes.inherit = FALSE, data = v_inla_df, aes(x=x, y=y), col ='red') + 
  ggtitle("") +
 theme(plot.title = element_text(hjust = 0.5))

for (i in 2:3){
  u_inla_df = as.data.frame(u_inla[[i]])
v_inla_df = as.data.frame(v_inla[[i]])

u_plots[[i]] = ggplot(data.frame(samp = samples$u[comp[i],-burnins]), aes(samp)) + 
  geom_histogram(aes(y=..density..), binwidth = binw) + 
  geom_path(aes.inherit = FALSE, data = u_inla_df, aes(x=x, y=y), col ='red') + 
  ggtitle("") +
 theme(plot.title = element_text(hjust = 0.5)) + xlab('x')

v_plots[[i]] = ggplot(data.frame(samp = v_MCMC[comp[i],-burnins]), aes(samp)) + 
  geom_histogram(aes(y=..density..), binwidth = binw) + 
  geom_path(aes.inherit = FALSE, data = v_inla_df, aes(x=x, y=y), col ='red') + 
  ggtitle("") +
 theme(plot.title = element_text(hjust = 0.5)) + xlab('x')
}

library(gridExtra)
grid.arrange(grobs = u_plots, ncol = 1)
grid.arrange(grobs = v_plots, ncol = 1)

```





```{r}
u_median = INLAOral$summary.random$Region.struct$`0.5quant`
germany.plot(exp(u_median), col=col, legend=TRUE)
```


##b)
We will now add some explanatory factors for getting oral cancer.

```{r}
smoking = read.table("additionalFiles/smoking.dat")

OralData["smoking"] = smoking

```

```{r}

INLAOSmoking1 = inla(
  formula = Y ~ f(Region.struct, model = "besag", graph = g, hyper = list(prec=list(prior="loggamma", param=c(alpha_u,beta_u))), constr = FALSE) + 
    f(Region, model = "iid", hyper = list(prec=list(param=c(alpha_v,beta_v))))-1 + smoking, 
 family = "poisson",
 E=E,
 data = OralData
)

```

```{r}

INLASmoking2 = inla(
  formula = Y ~ f(Region.struct, model = "besag", graph = g, hyper = list(prec=list(param=c(alpha_u,beta_u))), constr = FALSE) + 
    f(Region, model = "iid", hyper = list(prec=list(prior="loggamma", param=c(alpha_v,beta_v)))) + f(smoking, model = 'rw2', hyper = list(prec = list(prior = "loggamma")), constr = FALSE)-1, 
 family = "poisson",
 E=E,
 data = OralData
)
```

