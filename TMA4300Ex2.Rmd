---
title: "TMA4300Ex2"
author: "Karine H. Foss & August S. Mathisen"
date: "15 3 2019"
output: pdf_document
---

#Introduction
```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)

```

```{r, eval=FALSE, include=FALSE}
#Installing packages
#install.packages(c("fields","spam"))
#install.packages("INLA", repos = c(getOption("repos"),
#                 INLA="https://inla.r-inla-download.org/R/stable"), #dep=TRUE)
```

```{r, child = 'text/introduction.Rmd'}
```


```{r}
#Loading libraries
library(ggplot2)
library(gridExtra)
library(coda) #For effectiveSize
library(spam) # load the data
str(Oral) # see structure of data
attach(Oral) # allow direct referencing to Y and E

# load some libraries to generate nice map plots
library(fields, warn.conflict=FALSE)
library(colorspace)

#Load the file with a function for evaluating multivariate normal densities parameterized on canonical form
source('additionalFiles/dmvnorm.R')
#Load the matrix that describes which districts in Germany that lie close to one another
load('additionalFiles/tma4300_ex2_Rmatrix.Rdata')

col <- diverge_hcl(8) # blue - red
# use a function provided by spam to plot the map together with the mortality rates
```

```{r, fig:germany_y, fig.width=5, fig.height=4, fig.cap="\\label{fig:germany_y} Distribution of cases of oral cancer across regions in Germany."}

germany.plot(Oral$Y/Oral$E, col=col, legend=TRUE)

```
Figure \ref{fig:germany_y} shows the ratio between the observed number of cases of oral cancer and the expected number based on demographics, that is $y_i/E_i$ for all districts in Germany. The general tendency is slightly higher rates than expected in the north east and south west and lower than expect in the mid east.
#Exercise 1: Derivations
```{r, child = 'text/task1.Rmd'}
```

#Exercise 2: Implementation of the MCMC sampler
```{r, child="text/task2theory.Rmd"}
```

```{r}
#Define functions to be used in the MCMC

#The c-function from the Taylor expansion of f in task 1b)
c_func = function(z, E){
  exp(z)*E
}

#The b-function from the Taylor expansion of f in task 1b)
b_func = function(z, y, E){
  y + c_func(z,E)*(z-1)
}

#A function that draws from the full conditional of kappa_u at a given MCMC step
drawKappaU = function(n, alpha_u, beta_u, u, R){
  rgamma(1, shape = (n-1)/2 + alpha_u, rate = beta_u + 0.5*t(u)%*%R%*%u)
}

#A function that draws from the full conditional of kappa_v at a given MCMC step
drawKappaV = function(n, alpha_v, beta_v, eta, u){
  rgamma(1, shape = n/2+alpha_v, rate = beta_v+0.5*sum((eta-u)^2))
}

#A function that draws from the full conditional of u at a given MCMC step
drawU = function(n, kappa_u, kappa_v, eta, R){
  t(rmvnorm.canonical(1, kappa_v*eta, kappa_u*R+diag.spam(kappa_v, n, n)))
}

#A function that draws from an approximation to the full conditional of eta at a given MCMC step. It is a multivariate normal approximation around z.
drawEta = function(n, z, y, kappa_v, kappa_u, u, E){
  t(rmvnorm.canonical(1, kappa_v*u + b_func(z,y,E), diag.spam(as.vector(c_func(z,E)))+ diag.spam(kappa_v, n, n)))
}

#A function that calculate the logarithm of the full conditional of eta up to a normalisation constant. It is used for calculating the acceptance probability.
logFullCondEta = function(eta, kappa_v, u, y, E){
  -kappa_v*0.5*t(eta)%*%eta+t(eta)%*%u*kappa_v + t(eta)%*%y-t(exp(eta))%*%E
}


```

Now we implement the MCMC algorithm. It updates each of the components at a time. That is, it follows the Gibbs sampler. For $\boldsymbol\eta$, a metropolis-hastings step is needed, as the full conditional is not on a form that can easily be sampled from in itself. The proposal density is a gaussian density found from Taylor expansion around the value in the previous step.
```{r}
#MCMC function
MCMCOral = function(M, y, starting_values, alpha_u, beta_u, alpha_v, beta_v, E, R){
  #Get computing time
  time_start = Sys.time()
  
  #Fetching starting values
  eta_prev = starting_values[['eta']]
  u = starting_values[['u']]
  kappa_u = starting_values[['kappa_u']]
  kappa_v = starting_values[['kappa_v']]
  
  n = length(y)
  
  #Create matrices to contain all steps
  eta_values = matrix(nrow = n, ncol = M)
  u_values = matrix(nrow = n, ncol = M)
  kappa_u_vals = vector(length = M)
  kappa_v_vals = vector(length = M)
  
  #Put the starting values in the first column of the matrices and vectors
  eta_values[,1] = eta_prev
  u_values[,1] = u
  kappa_v_vals[1] = kappa_v
  kappa_u_vals[1] = kappa_u
  
  acceptance_rates = vector(length = M-1)
  num_accepted = 0
  
  for(i in 2:M){
    #Generate next step for u and kappas
    kappa_u = drawKappaU(n, alpha_u, beta_u, u, R)
    kappa_v = drawKappaV(n, alpha_v, beta_v, eta_prev, u)
    u = drawU(n, kappa_u, kappa_v, eta_prev, R)
    
    #prpose eta
    eta = drawEta(n, eta_prev, y, kappa_v, kappa_u, u, E)
    
    #generate a random number to determine wether to accept or reject
    decision_var = runif(1)
    
    #Calculate the different terms that go into the expression for the acceptance probability
    p_eta = logFullCondEta(eta, kappa_v, u, y, E)
    p_eta_prev = logFullCondEta(eta_prev, kappa_v, u, y, E)
    q_eta = dmvnorm.canonical(eta, kappa_v*u + b_func(eta_prev,y,E), diag.spam(as.vector(c_func(eta_prev,E)+kappa_v)), log=TRUE)
    q_eta_prev = dmvnorm.canonical(eta_prev, kappa_v*u + b_func(eta,y,E), diag.spam(as.vector(c_func(eta,E))+kappa_v), log=TRUE)
    
    #Calculate the logarithm of the acceptance probability
    log_acceptance = p_eta-p_eta_prev+q_eta_prev-q_eta
    
    
    #Using log_acceptance further
    acceptance_rates[i] = min(1, exp(log_acceptance))
    
    #Determine wether to accept or reject
    if(decision_var<acceptance_rates[i]){
      eta_prev = eta
      num_accepted = num_accepted +1
    }
    
    #Update value matrix
    eta_values[,i] = eta_prev
    u_values[,i] = u
    kappa_v_vals[i] = kappa_v
    kappa_u_vals[i] = kappa_u
  }
  time_end = Sys.time()
  #Return all values in a list
  return(list(eta = eta_values, u = u_values, kappa_u =kappa_u_vals, kappa_v=kappa_v_vals, acceptance_rates =  acceptance_rates, num_accepted=num_accepted, time = difftime(time_end, time_start,units = 'secs')))
}
```

Known values, the number of samples and initial values are set before we run the sampler. Note that the initial values for the $\kappa$s are not necessary, as they are never used in the sampler. However, they are here given as their prior expected values, which is where we would assume the values to be before any data is obtained.
```{r}
#Set parameters
M = 50000
y = Oral$Y
E = Oral$E
alpha_u = 1
alpha_v = 1
beta_u = 0.01
beta_v = 0.01

#Set initial values
n = length(y)
u_start = matrix(0,ncol = 1, nrow = n)
eta_start = matrix(0,nrow = n, ncol=1)
kappa_u_start = 100
kappa_v_start = 100
starting_values = list(eta = eta_start, u = u_start, kappa_u = kappa_u_start, kappa_v = kappa_v_start)
```

```{r, eval = TRUE, cache = TRUE}
#Run function.

#set.seed(4300)
#samples = MCMCOral(M,y,starting_values,alpha_u,beta_u,alpha_v,beta_v,E,R)

#Save and load, so we don't need to do it over and over again
#save(samples, file = 'MCMCsamples.RData', ascii=TRUE)
load('MCMCsamples.RData')
M = length(samples$kappa_u)
```


#Exercise 3: Convergence diagnostics
We will now do some analysis of the result of the MCMC. First, we will obtain some diagnostic summaries for the parameters. Since we have obtained results for $\kappa_u$, $\kappa_v$, $\boldsymbol u$ and $\boldsymbol\eta$, these are the ones we will analyse. The aim is to try to conclude whether or not the chain has converged to the target/stationary distribuition.

##a) Trace plots
First, we look at the trace plots for $\kappa_u$ and $\kappa_v$. 

```{r, fig:trace_kappa1, fig.width=5, fig.height=4, fig.cap="\\label{fig:trace_kappa1} Trace plots for kappa. First 100 samples. Here, the burn-in-period is clearly visible.", echo = FALSE}
#x
#Trace plots Kappa
start = 1
stop = 100
trace.kappa_u = 
  ggplot(data.frame(kappa = samples$kappa_u[start:stop]), aes(x=start:stop, y = kappa)) + geom_line()+ ggtitle("MCMC trace plots: Kappa") + theme(plot.title = element_text(hjust = 0.5)) + xlab('Iterations') + ylab('kappa_u')

trace.kappa_v = 
  ggplot(data.frame(kappa = samples$kappa_v[start:stop]), aes(x=start:stop, y = kappa)) + geom_line() + xlab('Iterations') + ylab('kappa_v')

trace.kappa_plot = grid.arrange(grobs = list(trace.kappa_u, trace.kappa_v), ncol = 1)
```


```{r, fig:trace_kappa2, fig.width=5, fig.height=4, fig.cap="\\label{fig:trace_kappa2} Trace plots for kappa. Burn-in-period is disregarded.", echo = FALSE}
#y
#Trace plots Kappa
start = 1000
trace.kappa_u = 
  ggplot(data.frame(kappa = samples$kappa_u[start:M]), aes(x=start:M, y = kappa)) + geom_line()+ ggtitle("MCMC trace plots: Kappa") + theme(plot.title = element_text(hjust = 0.5)) + xlab('Iterations') + ylab('kappa_u')

trace.kappa_v = 
  ggplot(data.frame(kappa = samples$kappa_v[start:M]), aes(x=start:M, y = kappa)) + geom_line() + xlab('Iterations') + ylab('kappa_v')

trace.kappa_plot = grid.arrange(grobs = list(trace.kappa_u, trace.kappa_v), ncol = 1)
```


```{r, fig:trace_kappa3, fig.width=5, fig.height=4, fig.cap="\\label{fig:trace_kappa3} Trace plots for kappa. Only 500 samples are shown. This means it is possible to see that the MCMC moves slowly is not mixing very well.", echo = FALSE}
#z
#Trace plots Kappa
start = 1000
stop = 1500
trace.kappa_u = 
  ggplot(data.frame(kappa = samples$kappa_u[start:stop]), aes(x=start:stop, y = kappa)) + geom_line()+ ggtitle("MCMC trace plots: Kappa") + theme(plot.title = element_text(hjust = 0.5)) + xlab('Iterations') + ylab('kappa_u')

trace.kappa_v = 
  ggplot(data.frame(kappa = samples$kappa_v[start:stop]), aes(x=start:stop, y = kappa)) + geom_line() + xlab('Iterations') + ylab('kappa_v')

trace.kappa_plot = grid.arrange(grobs = list(trace.kappa_u, trace.kappa_v), ncol = 1)
```


Figure \ref{fig:trace_kappa1} shows the first $100$ samples, so that the burn-in period is displayed. In figure \ref{fig:trace_kappa2} the burn-in period (or actually, the first $1000$ samples) is disregarded, to better show the variability in the parameters in the (hopefully) stationary distribution, and figure \ref{fig:trave_kappa3} shows just a small part of the samples, and the individual steps in the algorithm is more visible here. Note that both $\kappa$s move quite slowly and that the mixing does not seem to be very good. Tt takes quite a lot of steps to explore the whole areas.


Since the vectors of $\boldsymbol\eta$ and $\boldsymbol u$ are of high dimensions, trace plots are only made for a few, randomly chosen components.

```{r}
#Trace plots somecomponents of u and eta
#Choose components: 
set.seed(4301)
comp = ceiling(n*runif(3))
```

```{r, fig:trace_u, fig.width=5, fig.height=4, fig.cap="\\label{fig:trace_u} Trace plots for three components of u.", echo = FALSE}
trace.u = list()
trace.eta = list()

#labels
lab = paste("comp", comp)

#Make first plots with title
  trace.u[[1]] = 
  ggplot(data.frame(u = samples$u[comp[1],]), aes(x=1:M, y = u)) + geom_line()+ ggtitle("MCMC trace plots: u components") + theme(plot.title = element_text(hjust = 0.5)) + xlab('Iterations') + ylab(lab[1])

trace.eta[[1]] = 
  ggplot(data.frame(eta = samples$eta[comp[1],]), aes(x=1:M, y = eta)) + geom_line()+ ggtitle("MCMC trace plots: eta components") + theme(plot.title = element_text(hjust = 0.5)) + xlab('Iterations')+ ylab(lab[1])

#Next plots do not need title
for (i in 2:length(comp)){
  trace.u[[i]] = 
  ggplot(data.frame(u = samples$u[comp[i],]), aes(x=1:M, y = u)) + geom_line() + xlab('Iterations')+ ylab(lab[i])

trace.eta[[i]] = 
  ggplot(data.frame(eta = samples$eta[comp[i],]), aes(x=1:M, y = eta)) + geom_line() + xlab('Iterations')+ ylab(lab[i])
}

trace.u_plot = grid.arrange(grobs = trace.u, ncol = 1)
```

```{r, fig:trace_eta, fig.width=5, fig.height=4, fig.cap="\\label{fig:trace_eta} Trace plots for three components of eta", echo = FALSE}
trace.eta_plot = grid.arrange(grobs = trace.eta, ncol = 1)
```

From figures \ref{fig:trace_u} and \ref{fig:trace_eta}, $\boldsymbol\eta$ and $\boldsymbol u$ seem to have very small burn-in periods, if any. To be on the safe side, in the further analysis the first $1000$ samples are disregarded for all parameters.
```{r}
#Define burn-in area
burnins = 1:1000
```

##b) Autocorrelation plots

Autocorrelation plots are made for the same components as the trace plots. These tell us how correlated the samples from the MCMC are.
```{r}
acf.kappa_u = acf(samples$kappa_u[-burnins], plot = FALSE)
acf.kappa_v = acf(samples$kappa_v[-burnins], plot = FALSE)
```

```{r, fig:acf_kappa, fig.width=5, fig.height=4, fig.cap="\\label{fig:acf_kappa} Autocorrelation function for the two kappas. We see that the steps are highly correlated, escpecially the precision parameter for v.", echo = FALSE}

plot1 = ggplot(data.frame(acf = acf.kappa_u$acf, lag = acf.kappa_u$lag), aes(x = lag, y = acf)) + geom_col(width = 0.2) + ggtitle("ACF plots: Kappa") + theme(plot.title = element_text(hjust = 0.5)) + ylab('acf kappa_u')

plot2 = ggplot(data.frame(acf = acf.kappa_v$acf, lag = acf.kappa_v$lag), aes(x = lag, y = acf)) + geom_col(width = 0.2) + ylab('acf kappa_v')

acf.kappa_plot = grid.arrange(grobs = list(plot1, plot2), ncol = 1)
```

```{r, fig:acf_u, fig.width=5, fig.height=4, fig.cap="\\label{fig:acf_u} Autocorrelation function for the three u components. The steps are somewhat correlated, but goes towards 0 after about 10-30 steps.", echo = FALSE}
#acf for u and eta
plot_u = list()
plot_eta = list()

#Make first plots with title
acf.u = acf(samples$u[comp[1],-burnins], plot = FALSE)
plot_u[[1]] = ggplot(data.frame(acf = acf.u$acf, lag = acf.u$lag), aes(x = lag, y = acf)) + geom_col(width = 0.2) + ggtitle("ACF plots: u components") + theme(plot.title = element_text(hjust = 0.5)) + ylab(lab[1])

acf.eta = acf(samples$eta[comp[1],-burnins], plot = FALSE)
plot_eta[[1]] = ggplot(data.frame(acf = acf.eta$acf, lag = acf.eta$lag), aes(x = lag, y = acf)) + geom_col(width = 0.2) + ggtitle("ACF plots: eta components") + theme(plot.title = element_text(hjust = 0.5)) + ylab(lab[1])

#Next plots do not need title
for (i in 2:length(comp)){
  acf.u = acf(samples$u[comp[i],-burnins], plot = FALSE)
  plot_u[[i]] = ggplot(data.frame(acf = acf.u$acf, lag = acf.u$lag), aes(x = lag, y = acf)) + geom_col(width = 0.2) + ylab(lab[i])

  acf.eta = acf(samples$eta[comp[i],-burnins], plot = FALSE)
  plot_eta[[i]] = ggplot(data.frame(acf = acf.eta$acf, lag = acf.eta$lag), aes(x = lag, y = acf)) + geom_col(width = 0.2) + ylab(lab[i])
}

acf.u_plot = grid.arrange(grobs = plot_u, ncol = 1)
```

```{r, fig:acf_eta, fig.width=5, fig.height=4, fig.cap="\\label{fig:acf_eta} Autocorrelation function for the three eta components. The steps are somewhat correlated, but goes towards 0 after about 10-20 steps.", echo = FALSE}

acf.eta_plot = grid.arrange(grobs = plot_eta, ncol = 1)
```

We see in figure \ref{fig:acf_kappa} that the MCMC steps for $\kappa_u$ and $\kappa_v$ are highly correlated. From the small steps in trace plot in figure \ref{fig:trace_kappa3}, this is not very surprising. Though the components for $\boldsymbol\eta$ and $\boldsymbol u$ also are correlated, these go towards zero a lot faster, as shown in figures \ref{fig:acf_eta} and \ref{fig:acf_u}.

##Convergence test
`geweke.diag()` performs a test for convergence of the markov chain.
It takes one fraction of the beginning of the chain and one fraction of the end of the MCMC. The burn-in period is disregarded. If the chain has converged to the stationary distribution, the two means should be equal. The geweke test statistic is asymptotically standard normal distributed.
The null hypothesis is that the chain has converged.

```{r, cache = TRUE}

#Make data frame of samples
u_df = as.data.frame(t(samples$u))
eta_df = as.data.frame(t(samples$eta))

#Apply geweke test: 
gew.kappa_u = geweke.diag(samples$kappa_u[-burnins])
gew.kappa_v = geweke.diag(samples$kappa_v[-burnins])

gew.u = geweke.diag(u_df[-burnins,])
gew.eta = geweke.diag(eta_df[-burnins,])


p_u = 2*pnorm(abs(gew.u$z), lower.tail = FALSE)
p_eta = 2*pnorm(abs(gew.eta$z), lower.tail = FALSE)
p_kappa = c(2*pnorm(abs(gew.kappa_u$z), lower.tail = FALSE),
          2*pnorm(abs(gew.kappa_v$z), lower.tail = FALSE))

```


```{r, echo = FALSE}
cat('kappa_u:')
print(gew.kappa_u)
cat( '  kappa_u:     test stat',gew.kappa_u$z, ' p-value', p_kappa[1], '\n')
cat( '  kappa_v:     test stat',gew.kappa_v$z, ' p-value',p_kappa[2], '\n')

cat('  u comp', comp[1], ': test stat', gew.u$z[comp[1]], ' p-value', p_u[comp[1]], '\n')
cat('  u comp', comp[2], ': test stat', gew.u$z[comp[2]],' p-value', p_u[comp[2]], '\n')
cat('  u comp', comp[3], ': test stat', gew.u$z[comp[3]],' p-value', p_u[comp[3]], '\n')

cat('eta comp', comp[1], ': test stat', gew.eta$z[comp[1]],' p-value', p_eta[comp[1]],'\n')
cat('eta comp', comp[2], ': test stat', gew.eta$z[comp[2]],' p-value', p_eta[comp[2]],'\n')
cat('eta comp', comp[3], ': test stat', gew.eta$z[comp[3]],' p-value', p_eta[comp[3]],'\n')

```

With a significance level of $0.05$, which is often used, we would conclude that $\kappa_v$ has not converges, but that the other components have. This is if we consider each component isolated. However, we are really interested in whether the chain as a whole has converged. To assess this question, we make a histogram of all p-values to see whether they seem to be uniformly distributed, as they would be if the null hypothesis is true and the chain has converged. This is shown in figure \ref{fig:p_vals}

```{r, fig:p_vals, fig.width=5, fig.height=4, fig.cap="\\label{fig:p_vals} P-values for the geweke test on each parameter component. They should ideally be uniformly distributed if the MCMC chain has converged.", echo = FALSE}
ggplot(data = data.frame(p_vals = c(p_u,p_eta, p_kappa), var = c(rep(1, length = length(p_u)), rep(2,length = length(p_eta)), rep(3, length = length(p_kappa))))) + geom_histogram(aes(x = p_vals, y = ..density..))

```

Overall, the p-values seem at least not far from uniformly distributed, though there might be slightly more low values that one would ideally have if the chain has converged. It should be noted, however, that the geweke test can only give some indication on whether the chain has converged or not. It would be easier to conclude that the chain had not converged if the values were very low, than it is to conclude that it has converged when the values vary. All we can say, is that there are no strong indications against convergence.

From the trace plots, it looks like the chain has converged. The autocorrelation plots show quite high correlation for some components, but some correlation cannot be avoided, as the proposed steps depend on the previous steps.
All in all, it seems the MCMC has converged, but of course, we cannot be completely sure. There might some unforeseen features that are not captured by the algorithm.

#Exercise 4: Effective sample set

Now we compute the effective sample size for the $\kappa$s.
```{r}
effSize.kappa_u = effectiveSize(samples$kappa_u[-burnins]); effSize.kappa_u
effSize.kappa_v = effectiveSize(samples$kappa_v[-burnins]); effSize.kappa_v
```

These values mean that out of the $M$ samples of the MCMC, we "effectively" have $939$ and $348$ samples from the posterior distributions of $\kappa_u$ and $\kappa_v$, respectively. These are quite low values, but as we have seen in the trace plots and acf, these MCMC chains move very slowly. To increase the effective sample size, we should try to make the algorithm more efficient and improve the mixing. One idea could be to perform block updates, where $\kappa_v$ and $\boldsymbol\eta$ were updated at the same time instead of sequentially.

```{r}
comp_time = as.numeric(samples$time)
#comp_time = 100 #For testing when the time was not computed
relEffSize.kappa_u = effSize.kappa_u/comp_time
relEffSize.kappa_v= effSize.kappa_v/comp_time
```

```{r, echo = FALSE}
cat('Relative effective sample size for kappa_u is ', relEffSize.kappa_u, '\n')
cat('Relative effective sample size for kappa_v is ', relEffSize.kappa_v, '\n')
```

The relative effective sample size is the effective sample size obtained per second of running the MCMC. This could be interesting, as it gives us some meaure of how efficient the MCMC algorithm is. Low computer time is not good in itself if the algorithm does not draw enough samples from the target distribution. Likewise, large effective sample size is of course desireble, but if the algorithm has to run for an unreasonable long time, some improvement of the algorithm or sampling approach should maybe be considered. 

These numbers in themselves might not be too informative, as they will depend on e.g. the computer, but in relation to other approaches for implementing the MCMC, they will give some measure of which to use. Still, it can be seen that relative effective sample sizes for the $\kappa$_s, in particular $\kappa_v$ are quite small, as the preceding analysis would indicate.

#Exercise 5: Interpretation of result


```{r, fig:germany_MCMC, fig.width=5, fig.height=4, fig.cap="\\label{fig:germany_MCMC} Posterior spatial dependency (exp(u)) of cases of oral cancer accross regions in Germany, obtained by MCMC."}
post_median = exp(apply(samples$u, 1, median))

germany.plot(post_median, col=col, legend=TRUE)
```

We see that the general tendencies from figure \ref{fig:germany_y} in the beginning of the report are also present in figure \ref{fig:germany_MCMC}, but that the values are a lot smoother accross regions. Figure \ref{fig:germany_MCMC} shows exp(median) of the posterior distribution for spatial correlation in number of oral cancer cases. We do not have any explanatory variables in our model, and the noise has expected value $0$. This means that the figure shows the posterior median of cancer cases accross regions divided by the expected number of cases for demographic reasons, $E_i$. 

#Exercise 6: INLA

##a)
R-INLA can also be used to estimate oral cancer across regions in Germany. First, we implement the same model as for the MCMC.
```{r}
#Build the same model using INLA
library(INLA)
g <- system.file("demodata/germany.graph", package="INLA")

#Known values and parameters (copied from task2)
y = Oral$Y
E = Oral$E
alpha_u = 1
alpha_v = 1
beta_u = 0.01
beta_v = 0.01
n = length(y)
OralData = cbind(Oral, Region = Germany$region, Region.struct = Germany$region)
```

```{r}
INLAOral = inla(
  formula = Y ~ -1 +
    f(Region.struct, model = "besag", graph = g, 
      hyper = list(prec=list(param=c(alpha_u,beta_u))), constr = FALSE) + 
    f(Region, model = "iid", 
      hyper = list(prec=list(param=c(alpha_v,beta_v)))), 
 family = "poisson",
 E=E,
 data = OralData,
 control.compute = list(dic=TRUE)
)

```

We compare the marginal distributions obtained by `inla()` to histograms for MCMC samples of the corresponding parameters. 

```{r}
#Marginal posteriors and histograms
kappa_u_marginal = INLAOral$marginals.hyperpar$`Precision for Region.struct`
kappa_v_marginal = INLAOral$marginals.hyperpar$`Precision for Region`
```


```{r, fig:kappa_inla, fig.width=5, fig.height=4, fig.cap="\\label{fig:kappa_inla} Posterior marginals from the estimation by INLA and corresponding histograms obtained by MCMC", echo = FALSE}
kappa_u_df = as.data.frame(kappa_u_marginal[-(74:75),])
kappa_v_df = as.data.frame(kappa_v_marginal[-(70:75),])
kappa_marginals_plot = list()

kappa_marginals_plot[[1]] = ggplot(data.frame(kappa = samples$kappa_u[-burnins]), aes(kappa)) + 
  geom_histogram(aes(y=..density..), binwidth = 0.5,boundary = 0) + 
  geom_path(aes.inherit = FALSE, data = kappa_u_df, aes(x=x, y=y), col ='red') + 
  ggtitle("Posterior marginals: kappa_u") +
 theme(plot.title = element_text(hjust = 0.5)) + xlab('kappa_u')


kappa_marginals_plot[[2]] =ggplot(data.frame(kappa = samples$kappa_v[-burnins]), aes(kappa)) + 
  geom_histogram(aes(y=..density..), binwidth = 10,boundary = 0) + 
  geom_path(aes.inherit = FALSE, data = kappa_v_df, aes(x=x, y=y), col ='red') + 
  ggtitle("Posterior marginals: kappa_v") +
 theme(plot.title = element_text(hjust = 0.5)) + xlab('kappa_v')

grid.arrange(grobs = kappa_marginals_plot, ncol = 1)
```



```{r}
u_inla = INLAOral$marginals.random$Region.struct[comp]
v_inla = INLAOral$marginals.random$Region[comp]
v_MCMC = samples$eta-samples$u
```

```{r, fig:u_inla, fig.width=5, fig.height=4, fig.cap="\\label{fig:u_inla} Posterior marginals from the estimation by INLA and corresponding histograms obtained by MCMC", echo = FALSE}

u_inla_df = as.data.frame(u_inla[[1]])
v_inla_df = as.data.frame(v_inla[[1]])

u_plots = list()
v_plots = list()
binw = 0.02

u_plots[[1]] = ggplot(data.frame(samp = samples$u[comp[1],-burnins]), aes(samp)) + 
  geom_histogram(aes(y=..density..), binwidth = binw) + 
  geom_path(aes.inherit = FALSE, data = u_inla_df, aes(x=x, y=y), col ='red') + 
  ggtitle("Posterior marginals: u components") +
 theme(plot.title = element_text(hjust = 0.5)) + xlab('u')

v_plots[[1]] = ggplot(data.frame(samp = v_MCMC[comp[1],-burnins]), aes(samp)) + 
  geom_histogram(aes(y=..density..), binwidth = binw) + 
  geom_path(aes.inherit = FALSE, data = v_inla_df, aes(x=x, y=y), col ='red') + 
  ggtitle("Posterior marginals: v components") +
 theme(plot.title = element_text(hjust = 0.5)) + xlab('v')

for (i in 2:3){
  u_inla_df = as.data.frame(u_inla[[i]])
v_inla_df = as.data.frame(v_inla[[i]])

u_plots[[i]] = ggplot(data.frame(samp = samples$u[comp[i],-burnins]), aes(samp)) + 
  geom_histogram(aes(y=..density..), binwidth = binw) + 
  geom_path(aes.inherit = FALSE, data = u_inla_df, aes(x=x, y=y), col ='red') + 
  ggtitle("") +
 theme(plot.title = element_text(hjust = 0.5)) + xlab('u')

v_plots[[i]] = ggplot(data.frame(samp = v_MCMC[comp[i],-burnins]), aes(samp)) + 
  geom_histogram(aes(y=..density..), binwidth = binw) + 
  geom_path(aes.inherit = FALSE, data = v_inla_df, aes(x=x, y=y), col ='red') + 
  ggtitle("") +
 theme(plot.title = element_text(hjust = 0.5)) + xlab('v')
}

grid.arrange(grobs = u_plots, ncol = 1)
```


```{r, fig:v_inla, fig.width=5, fig.height=4, fig.cap="\\label{fig:v_inla} Posterior marginals from the estimation by INLA and corresponding histograms obtained by MCMC", echo = FALSE}
grid.arrange(grobs = v_plots, ncol = 1)

```



Figure \ref{fig:kappa_inla} shows the posterior marginals for $\kappa_u$ and $\kappa_v$ obtained by INLA. They have approximately the same shape as the corresponding histograms from the MCMC. Corresponding plots for the components of $\boldsymbol u$ and $\boldsymbol v$ are shown in figures \ref{fig:u_inla} and \ref{fig:v_inla}. Here, the distributions seem to correspond very nicely, meaning that the two methods arrive at almost exactly the same distributions. $\boldsymbol v$ is shown and not $\boldsymbol \eta$, since this is what is obtained in INLA. Samples from $\boldsymbol v$ are easily obtained for the MCMC from $\boldsymbol v = \boldsymbol \eta - \boldsymbol u$. 

```{r, fig:germany_INLA, fig.width=5, fig.height=4, fig.cap="\\label{fig:germany_INLA} Posterior spatial dependency (exp(u)) of cases of oral cancer accross regions in Germany, obtained by INLA."}

u_median = INLAOral$summary.random$Region.struct$`0.5quant`
germany.plot(exp(u_median), col=col, legend=TRUE)
```

We see, when comparing the figures \ref{germany_MCMC} and figure \ref{germany_INLA}, that we get the same spatial structure (up to what can be spotted by the eye) using INLA and MCMC. 

##b)
We will now add smoke as an explanatory factor for getting oral cancer, both as a linear effect and a non-linear function using random walk.

```{r}
#Add smoking to the data set to be able to use it as a covariate
smoking = read.table("additionalFiles/smoking.dat")
OralData["smoking"] = smoking
```

```{r}
#Smoking as linear effect
INLASmoking1 = inla(
  formula = Y ~ 
    f(Region.struct, model = "besag", graph = g, 
      hyper = list(prec=list(prior="loggamma", 
                             param=c(alpha_u,beta_u))), constr = FALSE) + 
    f(Region, model = "iid", 
      hyper = list(prec=list(param=c(alpha_v,beta_v)))) +
    smoking - 1, 
 family = "poisson",
 E=E,
 data = OralData,
 control.compute = list(dic=TRUE)
)

```


```{r}
#Smoking as non-linear effect
INLASmoking2 = inla(
  formula = Y ~ -1 +
    f(Region.struct, model = "besag", graph = g, 
      hyper = list(prec=list(param=c(alpha_u,beta_u))), constr = FALSE) + 
    f(Region, model = "iid", 
      hyper = list(prec=list(prior="loggamma", param=c(alpha_v,beta_v)))) + 
    f(smoking, model = 'rw2', 
      hyper = list(prec = list(prior = "loggamma")), constr = FALSE), 
 family = "poisson",
 E=E,
 data = OralData,
 control.compute = list(dic=TRUE)
)
```

We want to compare the three methods (With and without smoking as covariate), using Deviance Information criterion (DIC), which is often used on hierarchichal Bayesian models for model selection.
One should prefer models with lower DIC.

```{r}
#Finding the DICs
DIC_spatial = INLAOral$dic$dic
DIC_smoking_linear = INLASmoking1$dic$dic
DIC_smoking_rw2 = INLASmoking2$dic$dic
```

```{r, echo=FALSE}
cat('DIC for the pure spatial model:', DIC_spatial,'\n')
cat('DIC for the linear smoking effect model:', DIC_smoking_linear,'\n')
cat('DIC for the rw2 smoking model:', DIC_smoking_rw2)
```

Comments on the DIC follow below. 
Finally, we plot the posterior median for the non-linear smoking effect together with the $95\%$ credible intervals for the different smoking levels.

```{r}
summary_smoking = INLASmoking2$summary.random$smoking

#Obtain median and quantiles for the credible intervals 
median_smoking = summary_smoking$`0.5quant`
lower_quant_smoking = summary_smoking$`0.025quant`
upper_quant_smoking = summary_smoking$`0.975quant`

#Get the corresponding smoking amount
ID_smoking= summary_smoking$ID
```


```{r, fig:median_smoking, fig.width=5, fig.height=4, fig.cap="\\label{fig:median_smoking} posterior median of the smoking effect (red line) with corresponding 95% credible interval (black lines).", echo = FALSE}
smoking_df = data.frame(index = ID_smoking, median = median_smoking, lower = lower_quant_smoking, upper = upper_quant_smoking)

ggplot(data = smoking_df, aes(x = index)) + 
  geom_path(aes(y = median), col = 'red') + 
  geom_path(aes(y = lower)) +
  geom_path(aes(y = upper)) +
  xlab('smoking amount') + ylab('effect of smoking') +
  ggtitle("Non-linear smoking effect") +
  theme(plot.title = element_text(hjust = 0.5))
```


According to the DIC criterion the best model for our data is the one with a linear smoking effect. The difference between this and the model with a non-linear smoking effect is on the other hand minimal, and the model without a smoking effect is also close behind. The plot of the non-linear effect, figure \ref{fig:median_smoking}, justifies this as it shows a clear trend from low smoking levels to high smoking levels. The effect is however close to linear, which means that the linear model will have similar predictions. In addition, zero is always contained in the $95\%$ credible interval which means that a model without an effect of smoking will also stay close to the models which account for it.


